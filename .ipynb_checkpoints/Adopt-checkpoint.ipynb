{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Dataset/train.csv')\n",
    "test = pd.read_csv('Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       pet_id           issue_date         listing_date  condition  \\\n",
      "0  ANSL_69903  2016-07-10 00:00:00  2016-09-21 16:25:00        2.0   \n",
      "1  ANSL_66892  2013-11-21 00:00:00  2018-12-27 17:47:00        1.0   \n",
      "2  ANSL_69750  2014-09-28 00:00:00  2016-10-19 08:24:00        NaN   \n",
      "3  ANSL_71623  2016-12-31 00:00:00  2019-01-25 18:30:00        1.0   \n",
      "4  ANSL_57969  2017-09-28 00:00:00  2017-11-19 09:38:00        2.0   \n",
      "\n",
      "    color_type  length(m)  height(cm)  X1  X2  breed_category  pet_category  \n",
      "0  Brown Tabby       0.80        7.78  13   9             0.0             1  \n",
      "1        White       0.72       14.19  13   9             0.0             2  \n",
      "2        Brown       0.15       40.90  15   4             2.0             4  \n",
      "3        White       0.62       17.82   0   1             0.0             2  \n",
      "4        Black       0.50       11.06  18   4             0.0             1  \n"
     ]
    }
   ],
   "source": [
    "print(train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Brown Tabby\n",
      "White\n",
      "Brown\n",
      "['ANSL_69903' '2016-07-10 00:00:00' '2016-09-21 16:25:00' 2.0\n",
      " 'Brown Tabby' 0.8 7.78 13 9 0.0 1]\n"
     ]
    }
   ],
   "source": [
    "# both train and test are missing a lot of data in the 'condition' column, so we're going to\n",
    "# fill in the missing data with imputer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(train[:,3:4])\n",
    "train[:,3:4] = imputer.transform(train[:,3:4])\n",
    "imputer.fit(test[:,3:4])\n",
    "test[:,3:4] = imputer.transform(test[:,3:4])\n",
    "\n",
    "\n",
    "print(type(train[0,4]))\n",
    "print(train[0,4])\n",
    "print(train[1,4])\n",
    "print(train[2,4])\n",
    "print(train[0,:]) # the one in the first 5 missing 'condition[2]'\n",
    "\n",
    "\n",
    "# this might not work bc condition is a 0.0, 1.0, 2.0, etc.... and the mean is apparently\n",
    "# 0.8833899867488622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Brown Tabby\n",
      "White\n",
      "Brown\n",
      "['ANSL_69903' '2016-07-10 00:00:00' '2016-09-21 16:25:00' 2.0\n",
      " 'Brown Tabby' 1.0300208994241908 -1.5107285279864824 1.1610133636188338\n",
      " 1.2572791543898296 0.0 1]\n"
     ]
    }
   ],
   "source": [
    "# the next step is to encode categorical data (most categorical data already has been, but for\n",
    "# whatever reason 'color_type'[4] hasn't)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [4])], \n",
    "                       remainder='passthrough')\n",
    "                       #sparse=False)\n",
    "#train = np.array(ct.fit_transform(train))\n",
    "#test = np.array(ct.fit_transform(test))\n",
    "\n",
    "print(type(train[0,4]))\n",
    "print(train[0,4])\n",
    "print(train[1,4])\n",
    "print(train[2,4])\n",
    "print(train[0,:])\n",
    "\n",
    "#this block is correct, getting weird errors when I try to fit_transform the CT\n",
    "\n",
    "# might need to convert dates/times to more numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANSL_69903' '2016-07-10 00:00:00' '2016-09-21 16:25:00' 2.0\n",
      " 'Brown Tabby' 1.0300208994241908 -1.5107285279864824 1.1610133636188338\n",
      " 1.2572791543898296 0.0 1]\n",
      "['ANSL_69903' '2016-07-10 00:00:00' '2016-09-21 16:25:00' 2.0\n",
      " 'Brown Tabby' 1.0300208994241906 -1.5107285279864828 1.161013363618834\n",
      " 1.2572791543898296 0.0 1]\n"
     ]
    }
   ],
   "source": [
    "# after we filled in missing data and encoded categorical data, we're going to scale everything\n",
    "# using the standardization method\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# gonna want to standardize the columns:\n",
    "# condition[3] ?\n",
    "# length[5]\n",
    "# height[6]\n",
    "# X1[7]\n",
    "# X2[8]\n",
    "# not sure if standardizing the dates would be helpful\n",
    "print(train[0,:])\n",
    "train[:, 5:9] = sc.fit_transform(train[:, 5:9])\n",
    "test[:, 5:9] = sc.transform(test[:,5:9])\n",
    "print(train[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 0.0 2.0 ... 1.0 1.0 1.0]\n",
      "[1 2 4 ... 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "# splitting the cleaned data into input/output\n",
    "# don't need pet_id, issue_date, listing_date, condition?\n",
    "# what about anonymous columns?\n",
    "X_train = train[:,0:9]\n",
    "y_trainB = train[:,9]\n",
    "y_trainP = train[:,10]\n",
    "print(y_trainB)\n",
    "print(y_trainP) # this checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be ready to start building our model\n",
    "# I'm thinking i'm going to use two different models, one to predict the pet_category, and one\n",
    "# to take pet_category into account and predict breed_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are our options\n",
    "# Logistic Regression -> more than two potential labels in both outputs, so not usable\n",
    "# KNN ->\n",
    "# Support Vector Machine ->\n",
    "# Naive Bayes -> \n",
    "# Decision Tree ->\n",
    "# Random Forest ->\n",
    "# ANN -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANSL_69903' '2016-07-10 00:00:00' '2016-09-21 16:25:00' 2.0\n",
      " 'Brown Tabby' 1.0300208994241908 -1.5107285279864824 13 9]\n"
     ]
    }
   ],
   "source": [
    "# model 1\n",
    "ann = tf.keras.models.Sequential()\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu')) # INPUT LAYER HANDLED AUTOMATICALLY\n",
    "# Dense is just output = activation(dot(input, kernel)  + bias)\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
    "# our first model is going to be predicting the pet_category, because the result will have a\n",
    "# direct influence on the breed_category. There are 4 categories, so we'll need 4 output\n",
    "# neurons for a onehotencoding type beat\n",
    "# the sigmoid activation function gives the probabilities of a binary output\n",
    "# for more than two categories use softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling model \n",
    "ann.compile(optimizer='adam', loss='categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/brendan/opt/anaconda3/envs/firstEnv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-a86129fcc3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trainP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/firstEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/firstEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2297\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2299\u001b[0;31m         raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m   2300\u001b[0m                            \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2301\u001b[0m                            'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "# training model\n",
    "ann.fit(X_train, y_trainP, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
